{
 "metadata": {
  "name": "",
  "signature": "sha256:206867970342bcacf41a15d8b33effd36e483c970905a9f1286a1bb5d8ea787b"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import sys\n",
      "sys.path.append(\"../../e2o_dstools/\")\n",
      "import e2o_calculateEvaporation"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# import message passing interface for python\n",
      "from mpi4py import MPI\n",
      "\n",
      "# import for memory use\n",
      "from pympler import tracker\n",
      "tr = tracker.SummaryTracker()\n",
      "tr.print_diff() \n",
      "\n",
      "\n",
      "comm = MPI.COMM_WORLD\n",
      "rank = comm.rank\n",
      "size = comm.size\n",
      "\n",
      "if rank == 0:\n",
      "    # todo = [(row, col) for row in range(nrow) for col in range(ncol)]\n",
      "    # Reorganize work a bit so we can scatter it\n",
      "    keyfunc = lambda x:x[0] % size\n",
      "    work = itertools.groupby(sorted(enumerate(list_pars), key=keyfunc), keyfunc)\n",
      "    # Data is now in the format:\n",
      "    # Expand the work so we get lists of row, col per node\n",
      "    workpernode = [[x[1] for x in val] for (key, val) in work]\n",
      "else:\n",
      "    workpernode = None\n",
      "\n",
      "\n",
      "# Now distribute the workload accross all processes\n",
      "workpernode = comm.scatter(workpernode, root=0)\n",
      "# workpernode = workpernode[0]\n",
      "#logging.info(\"Got the following work in process rank {} : {}\".format(rank, workpernode))\n",
      "\n",
      "# Each node can now do it's own work. The main advantage is that we can do a gather at the end to collect all results.\n",
      "# Keep track of all the runs per node in scores\n",
      "scores = []\n",
      "\n",
      "# before starting any runs, make sure that you know in which folder we run this MPI run routine. \n",
      "# Always return to this folder before the next run\n",
      "curdir = os.getcwd()\n",
      "for i, par_set in workpernode:\n",
      "    logging.info(\"rank %02.f computing scores for parameter set nr %04.f\" % (rank, i))\n",
      "\n",
      "    runId = '%s_%04.f' % (R, i)\n",
      "    keys=[]\n",
      "    values=[]\n",
      "    for key, value in par_set.items():\n",
      "        keys.append ( key.replace('\\r',''))\n",
      "        values.append(value)\n",
      "        par_set2 = dict(zip(keys, values))\n",
      "    parmult = '%s' %(par_set2)\n",
      "    argv = ['-C', caseFolder, '-R',runId, '-c', inifile, '-I', '-T', str(timeSteps), '-s 3600', '-P', parmult, '-l ERROR']\n",
      "#    argv = ['-C', caseFolder, '-R',runId, '-c', inifileNew, '-T', str(timeSteps), '-s 86400']    \n",
      "    # run model, perhaps return something to store in a list\n",
      "    wf.main(argv)\n",
      "    tr.print_diff()\n",
      "    scores.append(i)\n",
      "    # Now make sure that you return to the right directory, in case the model has changed the path\n",
      "    os.chdir(curdir)\n",
      "\n",
      "# Wait here so we can collect all runs\n",
      "# Because we distributed the work evenly all processes should be here at approximately the same time\n",
      "comm.Barrier()\n",
      "# Great, we're all here. Now let's gather the scores...\n",
      "# Collect values from all the processes in the main root\n",
      "scores = comm.gather(scores, root=0)\n",
      "# Only the root node should collect all the data\n",
      "logging.debug(\"Rank {} has scores {}\".format(rank, scores))\n",
      "\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}